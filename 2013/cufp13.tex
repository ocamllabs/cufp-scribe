%% \NeedsTeXFormat{LaTeX2e}
\documentclass{jfp1}

\usepackage[usenames,dvipsnames]{color}
\usepackage{xspace}
\usepackage{url}
\bibliographystyle{jfp}

\newenvironment{ipar}[0]%
 {\begin{list}{}%
 {\setlength{\leftmargin}{1cm}}%
\item[]%
 }
 {\end{list}}

\newcommand\needcite{{\color{red} [cite]}\xspace}
\newcommand{\note}[1]{ \begin{ipar}  {\color{Gray} \textit{Note}: #1} \end{ipar}}


\title{CUFP'13 Scribe's Report}

\author[Marius Eriksen, Michael Sperber and Anil Madhavapeddy]
       {MARIUS ERIKSEN\\
        Twitter, Inc.\\
        1355 Market Street, Suite 900 \\
        San Francisco, CA 94103, USA.\\
        MICHAEL SPERBER\\
         Active Group GmbH, Hornbergstra\ss{}e 49\\
         70794 Filderstadt, Germany\\
         ANIL MADHAVAPEDDY\\
        Computer Laboratory, University of Cambridge\\ 
        15 JJ Thomson Avenue, Cambridge CB3 0FD, UK}

% \jdate{September 2001, update April 2007}
% \pubyear{2001}
% \pagerange{\pageref{firstpage}--\pageref{lastpage}}
% \doi{S0956796801004857}

\begin{document}
\maketitle

\tableofcontents

\section{Overview}

The Commercial Users of Functional Programming workshop (CUFP) is an
annual workshop held in association with the International Conference
on Functional Programming (ICFP), which was held in Boston, MA, USA
Sep 22--24, 2013. Previous workshops have been reported in JFP~\cite{JFP:9147276,JFP:8514633}.

The aim of the CUFP workshop is to report on the use of functional
programming in commercial ventures. (To wit, our motto is ``functional
programming as a means, not an end.'')

This report aims only to adumbrate; curious readers may wish to peruse
the recorded videos from the workshop.\footnote{Available from
  \url{http://cufp.org/2013/}.} CUFP does not
publish proceedings.

This year saw a record number of submissions, a reflection of the
growing popularity of functional programming. The submissions were
also quite varied; topics ranged from implementing systems for serving
advertisement
in Erlang to medical device automation in Scheme.

\section{Keynote: 21s Century Crusades of Knights of the Lambda
Calculus â€” Lessons from Past Language Crusades}

Dave Thomas, former CEO of OTI, the company responsible for the
initial development of the Eclipse IDE, talked to us about his
experiences with the ``language wars'' in the past; namely, the
introduction of object oriented programming into the mainstream as
well as the advocacy of logic programming and expert systems. Thomas
delivered a talk sprinkled with anecdotes and history, much opinion,
as well as some advice to the the functional programming community.

%% notes.marius.txt:1

Thomas jokes that his qualifications include his proclivity to
``infect'' organizations with new technology which they had no
intention of actually using. The only reason he has been able to do
this is this is simple: it works. He is careful to introduce new
technologies when they help solve actual problems, when he can find a
team who can manage to execute, making full, productive use of
whatever technology they are introducing.

Next, Thomas propounded that FP is experiencing a surge in
popularity because of multicore
computing and ``big data'' applications. He warned, though, that 
the goal of a community should be to experience \textit{modest}
success as massive industrial successes usually creates a ``really ugly
winner,'' attendant with lots of ``really ugly code.'' (We assume he
was referring to Java and to C++.)

Thomas proceeded to give us a laundry list of important
considerations for a language to succeed commercially.

%% Note: This section was super rambly; I did my best to summarize.

\paragraph{Language interoperability} It is very important that your
language is able to make use of prior work. Most companies develop
large legacies; new technologies cannot operate in isolation.
The first commercial deployment of Smalltalk was for
Tektronix\footnote{See \url{http://www.tek.com/}} oscilloscopes; in this project, technologies from several
``cultures''---hardware, firmware, systems software, application
software---needed to integrate. This was successful only because 
the Smalltalk implementation was designed to interoperate in this
manner.

\paragraph{Have an end-to-end story} It is that a
a system be understandable, end-to-end. Systems must be
diagnosable, which is especially important when solving 
``space/time problems.'' Sophisticated JIT runtimes like Oracle's Hotspot
are antipatterns: they make the runtime behavior of a system too
difficult to understand.

\paragraph{Serialization and data/code portability} Serialization has
taken a prominent role in the modern computing environment. The 
increasingly distributed nature of most systems necessitate good
serialization.  Most of these problems
have been solved, and thus new systems should use prior art. Modern systems need tools that
can extract datafrom the computing environment; we need to have
fine-grained control of where this data lives. Distribution also demands
easy \textit{deployment}; a simple way to create deployable executables
is a requirement for a modern language.

\paragraph{Performance} Thomas asserted that ``computers like
rectangles,'' and advised language designers to put care and
thought into how arrays are handled and implemented. This would solve
many problems and also avoid the need for fancy optimization. Modern
architectures rely on caching for performance, thus control over data
locality is paramount as critical parts of your application must be
kept in cache. Thomas also advised against stack machines: ``Stacks
are easy to put together, but hard to make fast.'' Instead,
implementors should use
a register model for your virtual machine. Virtual machines also
need intrinsics to take advantage of modern hardware architectures.
Intrinsics also help future-proof virtual machines---hardware changes
very rapidly.

%% \paragraph{Types} 

\paragraph{Adoption} The main impediment to functional programming
adoption, Thomas claims, is that functional languages can \textit{make
users feel stupid}. While dealing with the already-daunting task of
understanding foreign ways of solving problems, the need to understand
comprehensions, folds, and monads only further alienate newcomers.
Functional languages tend to erect high barriers to entry for these
reasons. Our community needs to pay special attention to this problem
by focusing on affordances and education. We need to empathize with
users and understand that they change at different rates. Thomas made
a special note of a recent feature in Haskell that allows users to run
\textit{wrong} programs. This he considers a healthy development.

Much of the thrust of functional languages get in the way of
developers writing ``CRUD (create-read-update-delete) apps;'' the kind
of application most of the world's developers are busy writing. Finally, Thomas
submitted that perhaps we should make FP languages smaller and more
focused: ``What we need is a collection of little right languages
instead of a smaller collection of partly right and partly wrong
kitchen-sink languages.''

\section{Analyzing PHP Statically}

%% notes.anil.txt:239

Facebook has invested a significant amount of effort in their PHP
infrastructure. Julien Verlaguet talked to us about
Hack~\cite{Verlaguet:2014:Hack}, a statically typed dialect of PHP
used extensively at Facebook. Hack runs on Facebook's PHP virtual
machine, HHVM, and the compiler is implemented in OCaml.

Software development at Facebook emphasizes a rapid feedback cycle.
There should be little time wasted between saving a source file, and
having the results show up on the screen. Much of the HHVM team's
efforts are focused on this ``developer experience.''

Due to Facebook's very large deployment, performance is very important.
Even a small performance improvement, say improving CPU utilization 
by 1\%, can have significant cost impact. PHP has been difficult to optimize
due to its dynamic nature, requiring an ever-more sophisticated 
virtual machine.

Hack is compatible with PHP, and interoperates with legacy PHP
with no runtime penalty. Due to Facebook's enormous legacy code base, 
Hack is explicitly designed to be incrementally adopted. This is done
by allowing gradual typing. Hack also has type inference, minimizing
notational overhead, and further aiding adoption. Hack's type system
is interesting in its own right: it is flexible enough to handle a large
number of uses where dynamic typing was previously invaluable; at
the same time, it provides sufficiently strong static guarantees to catch
a significant number of errors during type checking.

In keeping with the stated goals, the response time from the compiler
is near instantaneous. Even with thousands of files, the type checker 
returns within a second. Julien demonstrated this to an impressed 
audience.

Facebook has also developed a web-based integrated development
environment (IDE) around Hack. In order to provide autocomplete and
code navigation features in the IDE, Hack's type checker is compiled 
to Javascript via \texttt{js\_of\_ocaml}~\cite{DBLP:journals/spe/VouillonB14} and run alongside the 
IDE by the web browser.

The Hack compiler uses a number of resident background processes. A
master process delegates work to a number of child processes; they
communicate via shared memory in a lock-free fashion. This
architecture allows Hack to eagerly type check files before a user
could even type the requisite commands. This is particularly important
when there are large changes, for example when switching git branches.

The vast amount of Hack's code is written in OCaml. The language was
chosen because it is ideal for symbolic computation, has excellent
performance, and can be compiled to Javascript.  The main
challenge with this choice is the lack of native multicore support, though
Facebook worked around this by engineering their own multiprocess
architecture.

\section{OpenX and Erlang Ads}

%% notes.anil.txt:389

OpenX is an advertising technology company. Their ad exchange was
written in PHP, but recently transitioned to Erlang. Anthony Molinaro
shared his perspective on this process.

As their platform grew, its weaknesses became quite apparent. In
addition to architectural issues --- a poor choice of databases, the
lack of HTTP load balancing --- the application runtime grew to be an
expensive aspect of OpenX's day-to-day operations. Additionally, OpenX
wanted to expand their product, requiring better support for
concurrent and low latency operations.

Molinaro decided that Erlang was a good fit for the problem space ---
highly concurrent systems with soft-realtime requirements --- and
prototyped an initial implementation within a few months. This experience
left him quite satisfied and he started to evangelize it to his coworkers.
The engineering team began to find more and more projects that
were suitable for Erlang:

\begin{enumerate}

\item OpenX moved away from Cassandra to a Riak-based data management
stack. (Riak is written in Erlang.)

\item The developers added Erlang-based services for various bits and pieces of their
software stack. A DSL for ad-selection was written so that application
logic could be interpreted by both Erlang and Java-based systems.

\item OpenX implemented a data-service layer, abstracting the database.

\item Erlang Solutions wrote an API router for OpenX
\end{enumerate}
%
OpenX currently has around 15 services written in Erlang, around 8 in
Java, and a mix of Python and PHP for frontend tasks.

Molinaro emphasized the importance of the architectural choices that
enabled the use of Erlang:

\begin{itemize}

\item OpenX's systems are entirely cloud based: they use generic hardware,
atuomated bootstrap, deployment via packages, and emphasize fault
tolerance.

\item OpenX's software infrastructure use many cross-language tools. Thrift~\cite{Slee:Thrift},
Protocol Buffers~\cite{Google:2014:Protobufs}, and Lwes all
contribute to a language-agnostic environment. An in-house system
called ``Framework'' provides scaffolding or code layouts and provides
support for building deployable packages from code. Framework also
enforces versioning and reproducibility across languages.

\item OpenX emphases on service-oriented architecture. Components are
single-purpose and loosely coupled.

\item Architectural choices enabled the use
of Erlang, but it was important to find a project with which to
showcase the technology.

\end{itemize}

\section{Redesigning the Computer for Security}

%% notes.anil.txt:510

Tom Hawkins spoke to us about the SAFE
project\footnote{\url{http://crash-safe.org/}} , which is funded by
DARPA. SAFE is a codesign of

\begin{itemize}
\item a new application language, Breeze;
\item a new systems programming language, Tempest;
\item a new operating system; and
\item a new processor 
\end{itemize}

The goal of SAFE is security at every level for defense in depth.
SAFE focuses on hardware enforced security, dynamic checking
in software being too expensive. The SAFE model requires fine-grained
information flow control, implemented in hardware.

Words of data in SAFE are called \textit{atoms} comprising 64 bits of 
metadata and 64 bits of payload. Atom metadata contains type and
label information, used by the hardware to perform access and integrity
checks. Every bit of data in SAFE belongs to an atom; metadata can always
be recovered.

Hardware and software were developed concominantly. While Breeze was
being designed, there was a need for an assembly language to drive the 
hardware, and to write low-level routines. The assembly language is quite
traditional, but with a few higher-level constructs to make programming
the SAFE architecture more convenient.

The team implemented an assembly EDSL in Haskell, using the host
language itself as a macro language. The EDSL is very much a typical;
using a monad captures the program description. 

The assembly EDSL was also used as a library to the Breeze compiler;
it worked seamlessly with the compiler's code generator.

However after two years, the Breeze language and compiler still was not in
a usable state, and writing the operating system in assembly language,
even with the help of a macro system, was not a compelling option. The
team concluded there was a need for a higher-level low-level language. Thus,
they started a new project called \textit{Tempest}.

Tempest is an imperative language with automatic register allocation,
and an optimizing compiler. It, too, uses the SAFE EDSL assembler
as a backend. Tempest is itself an EDSL in Haskell. This arrangement
made it quite simple to inline SAFE assembler.

Hawkins concluded with a handful of lessons learned:

\begin{itemize}

\item Designing a higher order language with information flow control
is hard. (A humorous aside: the optimal number of PL researchers on
any given project is somewhere between 2--7.)

\item The team would have been more productive by starting by constructing
Tempest initially, and not the SAFE assembler. While a valuable building block,
programming directly in assembly language is unproductive. Furthermore,
having a higher-level language isolates the software and hardware teams:
the hardware team can now change the instruction-set architecture
without rewriting all the software!

\item EDSLs are excellent for bootstrapping languages like this; they are also 
highly reusable components, as we have seen.

\item EDSLs do require that engineers are comfortable with the host language
(in this case, Haskell); and they are more difficult to debug.

\item Concrete syntax is still important. The question is when to best make
the switch. Hawkins asserted that the best opportunity to inroduce a concrete
syntax is when a language gains modularity, since then the switch can be made
without disruption.

\end{itemize}

\section{End to End Reactive Programming}

%% notes.anil.txt:663

Jafar Hussain from Netflix talked to us about their use of functional
programming techniques; in particular, reactive programming. Their
path towards reactive programming began by the process of
modularizing the Netflix software stack. The middle tier and UI
used to be highly coupled, leading to numerous problems, specifically very
inefficient call patterns between components since the software
architecture was not modular and forced developers into using
omnibus messages to communicate between the application tiers.

Developers at Netflix are roughtly dichotomized into two personas:
``Cloud'' people and ``UI'' people. Because of Netflix's rather
massive scale, ``thinking at scale'' must be ubiquitous, and not done
by only the backend infrastructure engineers. Their central challenge
was this: how do they get UI engineers to think at scale? How do they
provide the infrastructure to make them effective ``cloud
developers?''

Netflix answered the question by providing ``UI comforts:'' a reactive
API. The next challenge was then mining concurrency and
parallelism in a simple fashion.  Hussain stated that no developer can
be trusted with locks, that there must be better ways.

Hussain went on to describe the \texttt{Observable} monad, the central
datastructure with which Netflix compose their systems. Observable is
part of a Java port of Rx.NET done by
Netflix~\cite{Christensen:2013:Reactive}. Briefly, it is is a vector
version of the Continuation monad with the null-propagation semantics
of the Maybe monad and error propagation semantics of the Either
monad. It is composed in a functional fashion, and has clean
cancellation semantics. \texttt{Observable} can be used in either
synchronous or asynchronous settings.

At Netflix, \texttt{Observable} is used as the singular data structure
for both cloud and UI developers: it is applied with ease to problems
in either setting, and provides a uniform API around which
applications are structured. Hussain demonstrated this by showing
examples of how the API smoothes over the differences between cloud
and UI programming --- they really are the same! --- providing a
uniform structure for both. His first example was showing an
implementation of social notifications, a classic ``long polling''
example; the second was an autocomplete system for search.

Like other speakers, Hussain emphasized the need for evangelism.
He recommended practicing public speaking and a particular focus
on ``soft skills.'' In his own experience, within a short amount of time,
he honed his speaking skills and was able to clearly articulate the
benefits and importance of a particular technology to his target 
audience.

In addition to traditional forms of evangelism, Hussain's group
developed interactive training exercises to illustrate the benefits
of reactive programming. These exercises also helped developers gain
an intuition of how to use these new constructs. The group  also made
themselves available at (nearly) all hours of the day to any developer
with questions.

Netflix's libraries for reactive programming, both in Java and in 
Javascript are open source and publicly available.
\footnote{\url{https://github.com/Netflix}}

\section{Medical Device Automation using Message-Passing 
Concurrency in Scheme}

%% notes.anil.txt:817

Vishesh Panchal from Beckman Coulter Inc gave a talk on their use of
Scheme to automate a molecular diagnostic device. Its job is to
detect the presence of specific strands of DNA/RNA in a sample. It is
a complex beast: it contains a total of 19 boards, with temperature and
motor control, sensors, barcode readers and a
spectrometer. (Panchal showed us a video of such a device playing a
piece of music by actuating its motors just so!)

Operators use a thin, stateless client to interface with the device.
The server software is written in Scheme. It implements Erlang's
message-passing concurrency model, and provides pattern matching, and
supervisor structure, used to isolate hardware failures.

Scientists program the machine with a DSL, also written in Scheme.
Panchal found this use of Scheme very apposite. Its syntax is
exceptionally clear and simple; hygenic macros played a crucial role
in constructing the DSL; first class functions and continuations were
key; finally, Scheme's arbitrary precision arithmetic was important
for numeric computation.

The team constructed an
Erlang/OTP embedding in Scheme. Their library can create, inspect, and
update Erlang records; they also replicated the Erlang pattern
matching facility through macros.

The instrument server is decomposed into several processes, all
arranged in a supervisor tree. These all communicate by passing
messages; the server also has an event manager that logs events and
handles subscriptions to event streams.

Panchal enumerated a number of common runtime errors, and showed how
they are impossible in their system, or otherwise mitigated by the
use of supervisor trees, enabling principled handling of such
failures at every layer in their stack.

The flexibility of their diagnostic device is due to its
programmability. However, scientists shouldn't need the help of
software engineers to make use of the device. Thus, their DSL must be
usable by nonexpert scientists. They solved this by making their DSL
highly specialized: it provides constructs for specifying high-level
goals, familiar to scientists. Panchal showed a number of examples
where there was a more-or-less direct translation between a written
description of a process and their DSL.

Panchal concluded by enumerating their chief take-aways:

\begin{itemize}

\item Message passing proved a useful model for reasoning about
system semantics. Erlang/OTP's fault isolation lead to shorter
programs, written less defensively. Smaller programs means simpler
programs, and thus easier to test. However it is not a silver bullet:
In their experience, gen-servers can deadlock between each other.
Here it is crucial to use timeouts and supervisor trees to detect
these.

\item There isn't much prior art on supervisor structure; the team
had to invent new methodology.

\item Automated unit testing was crucial throughout the process.

\item It is difficult to hire into their unique environment.

\item Existing quality metrics (e.g. bug density) doesn't adapt well
to languages like Scheme due to their terseness.

\item There was additional scrutiny from the FDA (the regulating body
for medical devices in the United States) due to their unusual
architecture and use of open-source software more generally.

\item Mixing languages, glued together by DSLs and macros, allowed
the team to use the good bits from both Scheme (macros, arbitrary
precision math) and Erlang (message passing concurrency, OTP). The
use of DSLs allowed rapid prototyping by nonexpert developers.
\end{itemize}


\section{Enabling Microservice Architectures with Scala}

%% notes.anil.txt:936

Kevin Scaldeferri from the Gilt Groupe reported on their experience
building out a large system comprising a large number of small
services. The Gilt Groupe is an Internet clothing retailer
employing highly user-specific targeting. They employ several schemes
to drive sales, mostly centered around time- and quantity-limited
offerings. This makes their traffic very uneven, with massive spikes
around sales. This also implies that their revenue is distributed
along the same lines, and so stability during traffic spikes is an
infrastructure imperative.

As with Netflix and OpenX, the software system at the Gilt Groupe
previously was largely a monolithic Ruby-on-Rails application.
Scaldeferri explained that, with a growing
application, and a growing number of engineers, there were seemingly
intractable software engineering challenges with this model. As well,
the setup caused a number of production issues.

They decided to pursue a ``microservice'' architecture, splitting
their application into a large number of welf-defined, self-contained
services. The transition began by factoring out of the Rails
application a few core infrastructure systems. Within four years,
these numbered 300. Each microservice is written in Scala, and uses
HTTP to communicate with each other. During this transition, the
engineering group developed architectural support to support this
large number of services in production:

\begin{itemize}

\item \textit{Build system}: Scaldeferri's group extended Scala's 
Simple Build Tool (SBT) to abstract away build, configuration, and 
dependency management. They extended the tool (via plugins)
extensively to support their use.

\item \textit{Configuration} is stored in a ZooKeeper cluster, and
can be overriden locally. Configuration is deserialized into to Scala
data structures with strict validation.

\item Due to the complex set of dependencies --- both upstream and
downstream --- \textit{testing} remains challenging in microservice
architectures. (Scaldeferri remarks: ``It's difficult for developers to run 
300 services on their laptops.'') Gilt's code base uses the ``cake pattern'' --- 
which is enabled by Scala's trait mixin system --- extensively and
uses this in testing to fully or partially satisfy dependencies that would
otherwise be handled by another service in their production environment.

\item Finally, a system for doing \textit{continuous delivery} is an
important part of Gilt's deployment strategy. 20--30 services are
deployed automatically on a typical day.

\end{itemize}

Finally, Scaldeferri adumbrated on their use of ``reactive''
programming. Many aspects of Gilt's products require realtime
updating. (Example: they show the number of items left for sale.)
These are constructed using Play~\cite{Typesafe:2014:Play} and
Akka~\cite{Typesafe:2014:Akka} actors.

\section{Functional Infrastructures}

Antoni Batchelli from PalletOps described the Pallet platform for
infrastructure automation.\footnote{\url{http://palletops.com/}}
Pallet allows users to write programs that build and operate computing
environments, both locally in the cloud.  

Pallet provides abstractions to write \textit{plans} that describe
configuration actions independently of the target platform.  Pallet
ultimately translates those plans to shell scripts.  While a shell
script generated from a plan is specific to a particular target
platform, the plan itself is target-independent.  Pallet knows about
many Linux-/Unix-based operating systems and is able to generate
appropriate shell scripts from these abstract plan specifications.

Pallet is written in Clojure, and allows specializing the actions that
comprise plans by using Clojure's multimethods.  The central entity in
a Pallet configuration is a \textit{plan function}, which is a pure
function that generates a plan object.  The plan object is
inspectable, and allows the user to query it in various ways~-- for
example, the user can find out what actions it would trigger on a
particular platform.  Pallet also optimizes plans.  For example, it
can coalesce many small actions into a single larger ones if a target
operating systems supports this.  The user can assemble plans into
phases that run on servers, and abstract over servers to instantiate
entire families of similar installations.

Batchelli concluded by noting that, over time, Pallet put a growing
emphasis on data rather than functions, which allow inspection and
intrusive manipulation.  While the dynamic typing of Clojure enables
many parts of Pallet, Batchelli sometimes wishes for static typing.

%% notes.anil.txt:1028

\section{Realtime Mapreduce at Twitter}

%% notes.anil.txt:1041

Sam Ritchie of Twitter Inc. described Summingbird, a new open-source
system for computing aggregates in real time.

Summingbird is a declarative-style DSL embedded in Scala for expressing map/reduce-style
aggregates over streaming data. It aims to bridge the gap between
streaming and batch computation. Summingbird enables developers to write logic
once and deploy it in a combination of batch- and
streaming-computation systems. An important goal of Summingbird is to
improve developer productivity by solving the systems problems in one
place, so that the runtime (i.e. the Summingbird library ands its
underlying execution engines) handles efficient execution as well as
scaling resources usage up and down based on need.

The core datastructure for aggregation in Summingbird is an
``associative plus'' operation --- the Monoid. It is a very practical
datastructure for aggregation: due to their being associative,
computations over Monoids are trivially parallelizable. Ritchie
described a slew of common datastructures and aggregations that are
Monoids. These include Sets, Lists, Maps (whose keys are also
Monoids), HyperLogLog, BloomFilters, Moments, Count-Min sketch, and so on.

It is common to deploy Summingbird in a dual batch/realtime
configuration. The batch portion, working off of log files or ground
truth data, computes the aggregate up to a given timestamp; a realtime
streaming system maintains a sliding window of the same aggregate. At
serving time, clients query both of these stores, merging the result.
This style of deployment is desirable because it allows good
separation of concerns: the batch system aggregates over the entire
data set, optimizing for throughput; the streaming system has a much
smaller fixed window of computation, computing updates with lower
latency.

Ritchie then explained an example production use of Summingbird. When
showing a tweet, Twitter will show a list of web sites that embed this
tweet. This is based on impression data, and is ordered by popularity.
The feature is implemented in Summingbird by consuming events of
the form \texttt{(TweetId, (URL, Count))} representing that
the tweet named by \texttt{TweetId} was reached from a given website,
represented by \texttt{URL}, a given number of times, represented by 
\texttt{Count}. This fits neatly into the model of Summingbird, as
these tuples are trivially summable. In order to deal with the large
cardinality of web sites, they use a Count-Min sketch to 
reduce the memory requirements for keeping the counts.

\section{Functional Probabilistic Programming}

%% notes.anil.txt:1168

Avi Pfeffer of Charles River Analytics introduced the \textit{Figaro} language
for probabilistic programming~\cite{Pfeffer:2009:Figaro}. The aim of
functional probabilistic programming is to ``democratize'' buildling
probabilistic models.

Pfeffer started with a motivating example: suppose you have some
information (e.g. ``Brian ate pizza last night'') and you want to
answer questions based on this (e.g. ``is Brian a student or a
programmer?''), all while keeping track of the uncertainty of the
answers. The solution is to create a joint probability distribution
over the variables, assert the evidence, and then use probabilistic
inference to compute the answer.

A common approach to this is through generative models: variables are
generated in order, where later values may bind (depend on) previous
values. Developing such a model is not a simple task, and is still an
active area of research.

Expressions in functional probabilistic programming languages are
computations that produce values with uncertainty. Pfeffer illustrated
by expanding on the previous example:

\begin{verbatim}
let student = true in
let programmer = student in
let pizza = student && programer in
(student, programmer, pizza)
let student = flip 0.7 in
let programmer = if student flip(0.2) else flip(0.1) in
let pizza = if student && programmer flip(0.9) else flip(0.3)
(student, programmer, pizza)
\end{verbatim}

Such programs may be understood by \textit{sampling semantics}:
the program is run many times; each outcome of the program has
some probability of being generated. Thus the program defines a
probability distribution over outcomes. Since the language itself is
Turing complete, it is capable of expressing a wide range of models.

Figaro is an EDSL in Scala which allows for distributions over any
data type. It has an expressive constraint system, and an extensible
library of inference algorithms containing many popular algorithms
such as variable elimination. Due to its embedding in Scala, Figaro
is usable as a library on the JVM, allowing for Java compatibility.
Programs may also make use of the abstractions afforded
by the host language.

Figaro's cheap data type is called \texttt{Element[T]} --- a class of
probabilistic models of type \texttt{T}. A number of atomic elements
are defined (e.g. \texttt{Constant}, \texttt{Flip}, \texttt{Uniform}, and
\texttt{Geometric}); these are combined to form compound elements, 
for example \texttt{If (Flip(0.9, Constant(0.5), ..}.

Figaro uses a probability monad to track state, with \texttt{Constant}
representing the monadic unit, and \texttt{Chain(T, U)} the monadic
bind. Most of Figaro's elements are implemented in terms of this monad.

Figaro is open source and freely available~\cite{CRA:2014:Figaro}.

\section{Building a commercial development platform Haskell}

%% notes.anil.txt:1290

Gregg Lebovitz from FP Complete reported on the FP Haskell Center, 
a web-based IDE for Haskell.  FP Complete aims to improve Haskell
adoption by making it more accessible to more developers. They offer
``commercial grade'' tools and support in order to simplify development
in Haskell. They aim to support the Haskell community while doing so.

The FP Haskell Center is aimed to take all the hassle out of starting to
use Haskell. It is a ready-to-use integrated development environment
for Haskell that is both complete and easy to use.

The IDE comprises

\begin{itemize}

\item A web frontend, with no setup required;

\item a Haskell backend implementing project management;

\item integration with the compiler to achieve instant developer
feedback (error reporting); 

\item a help help/documentation system;

\item git-based version control;

\item a build system; and

\item an execution and deployment platform.

\end{itemize}

The IDE is itself built almost entirely in Haskell, using libraries
and frameworks, mostly as-is, available on Hackage.
Yesod~\cite{Snoyman:2012:Developing} was used as a web framework.
Fay\footnote{\url{https://github.com/faylang/fay/wiki}}, a proper subset of Haskell that compiles to Javascript,
was used for frontend code.

The challenging aspects of this project were largely related
to integration. Using the compiler as a library allowed for easy
and type-safe introspection of errors; a user's code is continuously
precompiled on the backend, so that its bytecode can be run
quickly within the IDE.

\section{Common Pitfalls of Functional Programming and How 
to Avoid Them:\\ A Mobile Gaming Platform Case Study}

%% notes.anil.txt:1353

Yasuaki Takebe of GREE, Inc. spoke about the use of functional
programming in mobile gaming. GREE is a large mobile gaming
platform in Japan (37 million users, 2000 games, 
2600 employees) who have historically built mobile games in
traditional web programming languages like PHP or Ruby.

Recently, GREE has begun using Haskell for some of its backend
systems. Takebe described one of these projects, a management system
for their in-house key-value storage system. Its job is to manage and
scale capacity in their storage clusters. It might, for example,
increase the cluster size due to hardware faults or to access spikes.

Takebe described a few incidents that were related to Haskell-specific
issues:

\begin{description}

\item[A memory leak due to lazy evaluation.] The frontend
server keeps a list of active threads in a \texttt{TVar} for
monitoring purposes. When threads were removed from the list, the
function was not reduced to normal form and thus created a large
memory leak.

\item[A race condition.] A race condition between dequeueing
items and an asynchronous exception thrown by a timeout handler caused
data to be lost. 

\item[A performance degradation.] The GREE team used a
library, \texttt{http-conduit}, to perform health checks of various
servers. In a minor version update, this library started to fork new
threads for each http request, leaving the caller with the
responsibility to perform explicit resource management. This caused
a resource leak in their code base; after a little while, the program ran
with as many threads as there had been health checks.
\end{description}
The GREE team 
decided to employ better testing practices to battle these problems. Using standard Haskell
tools (QuickCheck~\cite{Claessen:2011:Quickcheck},
HUnit~\cite{Herington:2014:HUnit}) they developed a harness within
which they could start their servers and test them in-situ. They now
have over 150 systems tests with more than 5000 assertions.

Next, they started to document the issues they ran into, in order to
share their experiences and prevent future mistakes of the same kind.
While they put significant effort into this, few developers bothered
reading them. Next, they started trying to enforce them via
HLint~\cite{Mitchell:2014:HLint}. While it is not sufficiently
expressive to detect all the issues they care about, it covers a
useful subset.

Finally, the group focused on more proactive education. They set up a
brown-bag lunch where they covered Haskell and Scala topics; they also
ran a class for new graduates where they sourced problems from project
Euler, solving them in Haskell.

Takebe considers their use of FP successful --- some of GREE's key
software is now written in a functional language. He paid particular
attention to the ``superstructure'' of a language: a community, good
documentation, good tooling. Takebe considers this the sine qua non
of introducing FP in a setting like GREE's.

\section{Building scalable, high-availability distributed systems in Haskell}

%% notes.anil.txt:1432

Jeff Epstein of Parallel Scientific spoke about the use of Haskell in
a high-availability (HA) distributed system for managing resources in
a large (10k+ nodes) environment --- a cluster manager. The cluster
manager also needs to be fully consistent, and has to recover from
failures quickly.

The team, for reasons undisclosed, determined that existing solutions,
ZooKeeper among them, were not equal to the task. They set out to
build their own Paxos~\cite{Lamport:1998:Part} implementation with Haskell.

The job of a cluster manager is to present a consistent view of the
cluster's state. While purely functional data structures were employed
--- the state of a cluster, for example, is represented by a purely
functional graph --- the code itself maintained an imperative feel.
This was important due to the inherently imperative nature of the
domain.

The group used Cloud Haskell~\cite{Epstein:2011:Towards} for
distribution. Cloud Haskell is a actor-style message-passing system,
similar to Erlang. It was a particularly good fit for this particular
project as its model --- of independent, communicating processes ---
meshes well with Paxos.

Paxos was implemented as a general purpose library on top of Cloud
Haskell. Each component of the algorithm --- the client, acceptor,
proposer, learner, and leader --- were all Cloud Haskell processes.
Their implementation turned out to be about 1.5kLOC of Haskell,
closely matching the pseudocode presented in the original Paxos paper.
This kind of near-transliteration provided great confidence in the
implementation's correctness.

While their initial implementation was a simple translation of
Lamport's original Paxos work, reality is inevitably more
complex. The team are working on adopting modifications available
in literature~\cite{Chandra:2007:Paxos} to improve performance.

Haskell's lazy evaluation was problematic, chiefly due to space leaks
in low-level networking code. Epstein noted that distribution is a
natural barrier to laziness since messages must be serialized across
process boundaries. Epstein cites strong typing as a great aid when
refactoring. Since it provides a platform for distribution, Cloud
Haskell makes it easy to develop and debug distributed systems on a
single machine. This setup greatly aided in development. Since such
systems generally, and Paxos especially, is sensitive to
nondeterminism, the group developed a deterministic scheduler for
Cloud Haskell. This allowed them to reliably and meaningfully test the
system and to reproduce errors.

\section{IQ: Functional Reporting}

%% notes.anil.txt:1525

Edward Kmett from S\&P Capital spoke about their use of functional
programming. First, Kmett recounted the introduction of Scala,
followed by a description of
\textit{Ermine}~\cite{Compall:2014:Ermine}, a new Haskell-like
language they developed for their domain.

Kmett first used Scala and FP techniques in a ``portfolio analytics''
engine. This is a product used for performance and risk attribution
across financial portfolios.

Here, they used monoids for simple parallelization. (This was also
seen in Twitter's report on Summingbird.) Reducers were used to
derive structure from containers.  Reduction
is done in parallel across monoidal structures. Kmett noted that this
enhanced compositionality means that ``we don't see the wiring hanging
out of our code any more.''

The old version of the portfolio analytics engine was written in Java.
It was hard to extend, and required all data to be in memory. Their
rewrite in Scala is much more elegant and extensible; it runs fast, in
constant space. This project really helped sell the use of Scala and
FP more generally to the company.

Next, Kmett talked about Ermine, the Haskell-like language they
developed to build a generic reporting and visualization framework to
be used in multiple products.

Ermine was developed because the authors were frustrated with Scala.
Specifically, writing monadic code in Scala can be quite painful as it
is easy to overflow the stack without trampolining. Ermine was developed
in both Scala and Haskell; a portable core enables it to run on the
JVM.

Ermine has a Haskell-like type system, but with row types, constraint
kinds, and rank-N types. Ermine has built-in database support. The
group also developed a structured code editor that prevents writing
code that does not type check.

Row types are very useful in this context as they provide a powerful mechanism
to describe the structure of data. The type system models constraints
with ``has'', ``lacks'' and ``subsumes''.

Finally, they built a declarative reporting layer which can push reports
into specific backends (e.g. SQL Server, HTML reports)

\section{Enterprise scheduling with Haskell at skedge.me}

%% notes.anil.txt:1593

Ryan Trinkle of skedge.me talked about the cloud-based scheduling
platform skedge.me. Skedge.me handles complex ``enterprise''
scheduling for, among others, retailers. For example, Sephora, a
makeup company, uses skedge.me to schedule appointments with
customers. Skedge.me is integrated into their site via an iframe; it
is styled seamlessly to look at part of the host site.

Skedge.me used to be a 43-KLoC application written in Groovy on Rails.
The codebase had several major intractable issues: timezone problems,
double bookings, recurring events not firing notifications, and
delayed notifications. The application also had severe performance
issues. Perhaps worst of all, the application was inflexible: they
could not respond easily to customers' needs, and even had to resort to
asking their customers to change the way they conducted business, so
that it might fit better with the skedge.me model.

After careful deliberation, the team decided to rewrite skedge.me in 
Haskell. Trinkle had worked with Haskell previously, though not
to build a web site.

The team began by constructing a monad, \texttt{RawDB}, to maintain
ACID guarantees during transactions. The \texttt{RawDB} layer tracks
effects and can automatically retry operations on temporary failure.

The \texttt{DB} monad is built on top of \texttt{RawDB} monad and 
provides a high-level ``CRUD'' interface. This layer makes heavy 
use of algebraic data types, and performs caching and validation.

The final layer before the application code is the security layer. It
implements security policies for various customers. Implementing
security turns out to be tricky due to the myriad ways in which the
product can be configured by customers. For example, they may define
roles (e.g. ``owner,'' ``staff,'' ``customer'') that make sense only
within their environment; the \textit{verbs} of the product, too, may
be configured (e.g. appointments may be joined or rescheduled)
depending on the environment. Thus both sides of the security equation
--- nouns and verbs --- can change from instance to instance.
Skedge.me uses Haskell's type class facility to model these security
policies. These help map customer-specific customizations into a
standard schema that can be manipulated on a component-by-component
basis. This technique affords the team a great deal of static
guarantees from type checking, a criticial property when implementing
security sensitive systems.

The team made heavy use of Hackage. Their application brings in 
71 unique libraries from Hackage. (An additional 87 are brought in
from the transitive dependency graph.) They found Hackage to be 
well organized. Because most libraries are purely functional in nature,
the team found them very easy to vet for quality. They replaced just
one library due to bugs. Trinkle made this observation: compared to 
the available libraries of other, more popular languages, Haskell's 
number fewer, but are of higher quality.

Trinkle finally noted that, while Haskell provides a great platform
for ``building code for the long run,'' the team also made good use of
the language for quick-and-dirty hacks. As an example, the program
that imports data from the old system was written in Haskell. It was
not written with modularity in mind --- it was, truly, a
quick-and-dirty-hack --- however, the type system still provides very
useful guarantees. Trickle noted that they couldn't have achieved
doing this ``as badly as they did'' without the aid of the type
system.

\section{Wolfram: Programming Map/Reduce in Mathematica}

%% notes.anil.txt:1689

Paul-Jean Letourneau of Wolfram closed this year's CUFP with his talk
on implementing MapReduce in Mathematica. Letourneau first described
the Mathematica language: everything is an expression; expressions are
transformed (rewritten) until they reach a fix-point. Expressions are
data structures (similar to Lisp's S-expressions).  As such Mathematica
follows a familiar LISP mantra: programs are data --- homoiconicity
abound. This allows a Mathematica program to manipulate expressions
--- e.g. to perform rebinding --- which is powerful for distribution.

Letourneau shared some examples of impressively short Mathematica
programs (``Everything is a one-liner in Mathematica~\ldots{} for a sufficiently
long line'' ---Theo Gray) including an image constructed recursively. 
Letournea described Mathematica as ``a gateway drug to declarative
programming.''

Next, Letourneau described
HadoopLink~\cite{Letourneau:2013:HadoopLink}, a system that integrates
Mathematica with Hadoop. HadoopLink allows for nearly seamless
distribution of Mathematica programs: mappers and reducers both are
ordinary Mathematica functions, stitched together by a Hadoop link
object for input and output. The programs can be defined on a single
page; HadoopLink takes care of the rest.

Letourneau then went on to build a simple genome search engine using
these primitives.  This problem lends itself particularly well to map/reduce
style computation. The program, including referring to large datasets,
was defined within a single Mathematica session; distribution was indeed
seamless.

Letourneau's talk was a fitting end to an excellent CUFP workshop.

\section{Conclusion}

From our purview, 2013 was a watershed year for CUFP. The breadth,
depth, and broad applicability of functional programming was on
display. This was true not only at the workshop itself, but also of a
deep bench of submissions we could not accept due to time constraints.

The program was rich in every dimension covered: techniques,
languages, and industries. We had talks from consumer Internet
companies, the biotech industry, the medical device industry, gaming,
and the financial industry. Languages in use varied from embedded
DSLs, to academic stalwarts, to home-grown languages. It seems that,
if anything, language considerations are taking a front-seat in modern
engineering practices.

Several talks discussed the role of functional programming in
\textit{service oriented architectures}. The process of decomposing a
monolithic application into many, smaller services has been a
fertile field for adopting functional programming. Teams
use the transition as an opportunity to reimplement smaller parts of
the system in new languages which are better suited to the problem at
hand. This proved a good strategy: the use of functional programming
is introduced in a controlled manner, gradually and carefully; its
advantages are made clear in a smaller setting without having to take
large risks.

We would like to thank Simon Thompson and Francesco Cesarini for
organizing this year's tutorials. Ashish Agarwal organized the evening
BoF sessions. We would also like the to thank the ICFP organizers for
their assistance in Boston. We look forward to seeing everyone again
at CUFP 2014 in Gothenburg!

\bibliography{cufp13}

\end{document}
