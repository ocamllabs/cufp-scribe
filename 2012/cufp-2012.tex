% This is file JFP2egui.tex
% release v1.02, 27th September 2001
%   (based on JFPguide.tex v1.11 for LaTeX 2.09)
% Copyright (C) 2001 Cambridge University Press

\NeedsTeXFormat{LaTeX2e}

\documentclass{jfp1}
\bibliographystyle{jfp}
\usepackage{url}

%%% Macros for the guide only %%%
\providecommand\AMSLaTeX{AMS\,\LaTeX}
\newcommand\eg{\emph{e.g.}\ }
\newcommand\etc{\emph{etc.}}
\newcommand\bcmdtab{\noindent\bgroup\tabcolsep=0pt%
  \begin{tabular}{@{}p{10pc}@{}p{20pc}@{}}}
\newcommand\ecmdtab{\end{tabular}\egroup}
\newcommand\rch[1]{$\longrightarrow\rlap{$#1$}$\hspace{1em}}
\newcommand\lra{\ensuremath{\quad\longrightarrow\quad}}

\title[Commercial Users of Functional Programming 2012]
      {Commercial Users of Functional Programming Workshop Report}

\author[Michael Sperber and Anil Madhavapeddy]
       {MICHAEL SPERBER\\
         Active Group GmbH, Hornbergstra\ss{}e 49\\
         70794 Filderstadt, Germany\\
         ANIL MADHAVAPEDDY\\
        Computer Laboratory, University of Cambridge\\ 
        15 JJ Thomson Avenue, Cambridge CB3 0FD, UK}

\jdate{May 2012}
\pubyear{2012}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}

\begin{document}

\label{firstpage}
\maketitle

\section{Overview}

Commercial Users of Functional Programming (CUFP) is a yearly workshop
that is aimed at the community of software developers who use functional
programming in real-world settings.  This scribe report covers the talks
that were delivered at the 2012 workshop, which was held in association
with ICFP in Copenhagen.  The goal of the report is to give the reader
a sense of what went on, rather than to reproduce the full details
of the talks.  Videos and slides from all the talks are available online at \url{http://cufp.org}.

\section{Keynote: Adopting Functional Programming}

Kresten Krab Thorup, CTO of Trifork, Aarhus gave the keynote address,
and took us on the voyage he had taken from being on ``object head''
to ``Erlang land.''  Thorup's foundational training in software
development was all in terms of object-oriented methodologies.  He
went on to work on Objective C for NeXT, afterwards took his Ph.D.\
and then founded Trifork, an IT services company with now about 250
employees that develops software solutions, provides training, and
organizes several well-respected conferences.

While Trifork originally capitalized almost exclusively on its Java
expertise, it now successfully applies Erlang in large-scale
industrial projects.  Thorup described (taking cues from anthropology)
how many organizations have not been able to make such transitions
easily: Groups tend to gather around an idea that keeps them together,
and try to keep new ideas at bay.  This makes it difficult for
long-time OO developers to adopt functional programming.

Trifork managed to stay flexible by making learning about new ideas
and communicating them part of their regular operation: Everyone at
Trifork should spend 10\% of their time in the structured exchange of
knowledge, by giving presentations, organizing meetings, give training
classes, or organizing conferences.

Thorup reviewed object-oriented programming and the ecosystem around
it to show how it had become successful: Through an intuitive idea~--
``an object is an independent encapsulated entity that inteprets
inputs on its own account''~-- but also because of the availability of
thinking tools: graphical notation for design, tools for mapping those
designs to programs, books on comon problems akin to ``Design
Patterns''~\cite{GammaHelmJohnsonVlissides1995}, analysis methods for producing systems,
and standardized qualification processes.

However, Thorup sees a series problem with the object-oriented model,
as objects have no coherent model of time, and no good way to compose
behaviors over time.  With the rise of multicore and distributed
computing, these become increasingly important.  Erlang, supporting
functional programming and an actor model for concurrency, parallelism
and distribution, addresses this issue.  Thorup stressed that Erlang
is not primarily a functional programming language, but that
functional programming helps Erlang meet its requirements: being a
language for writing robust distributed applications.

Thorup described large-scale projects done using Erlang: One for
managing health-care records in Denmark, and one for sharing data
among ``sometimes connected devices'' such as a cell phones at a music
festival.

Thorup concluded by noting that two fundamental classes of problems in
software development require different classses of solutions:
\textit{Interactive systems} with multiple parties are fundamentally
stateful, and where developers should understand the handling of
state~-- for those problems, actors are a good model.
\textit{Transformational systems} map input to output, where
developers want to abstract over the details of hardware utilization,
the handling of mutable state and coordination.

\section{Jane Street Status Report}

Jane Street is a quantitative proprietary trading firm that has been a
well-known user and supporter of OCaml.  Yaron Minsky reviewed the
past decade of OCaml use at Jane Street.

Jane Street has three kinds of requirements on their own software:
\begin{description}
\item[Correctness] Jane Street trades billions of dollars every day~--
  much more than the company is worth.  Hence, a software error can
  have disastrous consequences.
\item[Agility] Jane Street needs to be able to adapt the software
  quickly to exploit new market opportunities as they are found.
\item[Performance] The software needs to quickly to exploit market
  opportunities.
\end{description}
%
Jane Street has written OCaml code for a number of application areas:
research tools for investiagting trading strategies, trading systems
(replacing legay systems written in VBA/Excel) that make trades
automatically, order gateways that implement protocols to interact
with markets, post-trade software to analyze and clean up completed
trades, systems infrastructure to manage systems, development tools,
trading tools, tools for managing market-data and desk infrastructure.

Minsky could not remember a crucial error in the software that
happened in production: Code at Jane Street generally fails by turning
off the system, which is acceptable in this environment.  Jane
Street's user interfaces are all text-based, and written using
Curses~-- this ``prevents bad UX designers from doing UX.''

Another important that favors OCaml is how the type system eases large
refactoring projects: The type system catches most mistakes that occur
in the middle of such projects.  The type system helps programmers
write readable, explicit code, which is more important than great
productivity increases or extremely concise programs.

Jane Street has developed a number of generally useful libraries for
OCaml, many of which are open
source.\footnote{http://janestreet.github.io/} In particular,
\textit{Core} is an enhanced ``standard library,'' meant to the
minimalistic libraries that ship with OCaml.  \textit{Async} is a
library with monadic concurrency abstractions, similar to available in
other functional languages such as
F\#~\cite{Syme:2011:FAP:1946313.1946334}.  \textit{Incremental}
describes large-scale computations with small updates.
\textit{Catalog} is a publish/subscribe system.  \textit{Nile} is a
distributed message-passing library.

In summary, Jane Street is fully committed to OCaml, and will continue
to do its development in OCaml.  Jane Street will continue to
contribute to open-source projects, and to collaborate with others
working on or in OCaml.

\section{Transmitting customised ads to set-top boxes with Erlang}

Macías Lopez, David Cabrero and Laura M.\ Castro from the University
of A Coruña reported on \textit{ADVERTISE}
project~\cite{Lopez:2012:DDA:2370776.2370800,
  Lopez:2012:FTC:2364489.2364498}.  \textit{ADVERTISE} is a
distributed system for transmission of customised ads to TV set-top
boxes via the TV network of a cable provider.  The system~-- entirely
written in Erlang~-- compiles events, emits ``advertising signals'' to
set-top boxes, and collects statistics about how many times specific
ads were displayed on consumer boxes.  ADVERTISE sends ads to more
than 100,000 clients.

Lopez reported on the difficulties of developing and deploying such a
large-scale system.  In particular, after the original system
implementation, the customer only provided hardware that did not
satisfy the original minimum specifications.  Moreover, the network
exhibits frequent node failures and netsplits, which disrupted the
operation of the original system.  Thus, while Erlang was a good
choice for implementing the system, merely using Erlang does not make
a distributed system robust in highly unreliable environments.
ADVERTISE yielded insights into best practices for implementing
distributed systems in such environments.

Netsplits in particular are problematic, as nodes may incorrectly
conclude other nodes are down, and then compete for control of the
network.  This may lead to data inconsistencies and duplicate
implementations of responsibilities assumed to be unique. An ADVERTISE
node, when it loses network connectivity, immediately suspends
execution and waits until it is restored, choosing consistency over
availability in such scenarios to avoid corrupting advertising
campaigns.

\section{Haskell for XenClient}

Matthias Görgens reported on using Haskell in the XenClient project at
Citrix: XenClient is a virtual-machine manager for clients, primarily
laptops in corporate and government environments, where XenClient
offers functionality different from the long-established XenServer,
such as trusted-computing support with hard-drive encryption and
native graphics performance.

The XenClient management stack consists of many daemons that commicate
via D-Bus and V4V (a VM-to-VM communications protocol).  Originally,
many of the daemons were written using Ruby, but a rising bug count
motivated the development team to look for alternatives, particularly
statically-typed languages to catch more bugs at development time.
Some daemons were then rewritten in Haskell.  By now, there are about
25,000 loc in Haskell in the system.  The XenClient daemons are
typically long-running, perform short bursts of communication and
computation upon a request discovered via polling, do not hold much
state and are restartable: Haskell is well-suited for this kind of
application.

As XenServer already uses OCaml for its management toolstack, it is a
bit surprising that XenClient chose Haskell: Görgens cited personal
preferences, the availability of more libraries, and the fact that
Haskell ``relieves pressure to share code with XenServer.''

Görgens also cited a few problems with using Haskell: XenClient found
it difficult to handle IO-heavy workloads, making Haskell work with
the OpenEmbedded build system in use in XenClient, and the
successfully train developers not yet familiar with functional
programming.

\section{Functional Programming @ Ghent IT Valley}

Romain Slootmaekers and Nicolas Trangez of Incubaid Research Lab
reported on using OCaml for implementing a distributed storage
service.  Incubaid Research Lab is an incubator laboratory for startup
companies.

In 2009, Amplidata~-- one of those spinoffs~-- was working on a
\textit{dispersed storage system} (DSS).  The system consists of a
multi-stage pipeline involving metadata storage, encoding and
decoding, storage management, and the backend disks.  At the time,
development on the storage-management component, written in C++, had
stalled: There were problems with resource management and threads, the
software had many bugs, and its object model had poor locality, which
led to poor performance.  Slootmaekers was able to (correctly)
re-implement the storage component in OCaml within two days, which led
to Amplidata considering using languages other than C++ for system
development.

The original reimplementation of the storage component had to be done
quickly, and was able to leverage OCaml's object-oriented substrate to
duplicate the architecture of the C++ original.  Also, the existence
of precise specifications and a test suite helped speed the rewrite.

The successor version of the system was then re-implemented mostly in
OCaml, and involved more leisurely and more complete refactoring.  The
rewrite used the \textit{Lwt}~\cite{Vouillon:2008:LCT:1411304.1411307}
library, and was delightfully painless.  Performance improved more
than twofold, and Amplidata was able to preserve code size while
adding significant features.  In particular, the newly written Arakoon
distributed key-value store was developed to hold the system's metadata.

OCaml helped developers through type inference, a fast compiler that
produces sufficiently fast code, the convenient C FFI, and the help
the type system gives when refactoring.  Downsides are poor tool
support, a scarce and fragmented library landscape, problems with
multicore support, that the object-oriented model which does not fit
well with the rest of the language.  Slootmaekers also cited the lack
of visual tools to talk about system architecture as a notable
problem.

In conclusion, Slootmaekers noted that any sufficiently large project
should be ready to use more than one language, and that functional
programming and distributed systems are a good match.

\section{From Streams to Functions (and Back Again)}

Frank McCabe of Starview, Inc.\ reported on Star, a new programming
language, that is part of Starview's event-processing platform.  (Star
is available by request the author via \url{www.star-lang.com}.)  The
purpose of Starview's platform is to provide \textit{operational
  intelligence}: to notice when a significant business event happened,
and to decide what to do about it, in real time.  The Star language
was developed to express this intelligence: It started out as
``StarRules,'' a simple language based on a ``on pattern do
something'' construct.  It could perform straightforward statistical
processing and infer significant events from event data.

StarRules's first application was scheduling in the semiconductor
industry.  In this domain, it is not enough for software to make
simple inferences from the data: It needs to make complex decision in
the face of an ever-changing environment, where machines frequently
break, invalidating any long-term plans.  The software needed here did
not fit StarRules's ``on pattern do'' construct.  Consequently,
Starview decided to turn StarRules into a general-purpose language,
and renamed it to Star.

A number of requirements shaped Star's design: Star needed to support
different programming styles in use at Starview.  The
semiconductor-fab scheduling application demanded safety, as errors in
the deployed software can be extremely costly.  Moreover, it needed to
satisfy modest real-time requirements: the software needs to be fast
enough to provide useful output before it is obsolete.

McCabe then made the following basic design decisions for Star: Star
is strongly to help with safety, but also to support tooling and help
communication in teams.  To avoid the bureaucracy of dealing with
types in Java, the language has type inference.  The type system is
based on algebraic types rather than objects, and has no
\texttt{null}.  (This decision was controversial within Starview,
which is still in many ways a ``Java house.'')

Star then amalgamated influences from various languages, in particular
functions as an organizing principle from other functional languages,
macros from April~\cite{fgm-klc:95} and Lisp to support syntatic
extensibity, as well as the Concurrent ML substrate for parallelism
and concurrency~\cite{Reppy1999}.  The type system borrows much from
Haskell: Star's type contracts are a variant of Haskell's type
classes~\cite{WadlerBlott1989}.  Star also includes influences from
Prolog and SQL.  Moreover, its actors based on \textit{speech
  actions}~\cite{searle:69} provide a mechanism for implementing
agent-like entities. Star is designed to be readable rather than
concise: This makes its texture distinct from other functional
languages such as Haskell or ML.

The combination of functions, macros and overloading provides a
coherent methodology for implementing DSLs in Star: A developer can
start with an ontological commitment in her problem domain, provide
appropriate syntax, which is translated into a macro invocation, which
is turned into function invocations, which are often backed by
contracts.  In particular, actors are implemented this way, as is
higher-level functionality for analytics and event processing.


\section{Functional Big-Data Genomics}

Ashish Agarwal reported on joint work with Sebastien Mondet, Paul
Scheid, Avid Madar, Richard Bonneau<sup>1, Jane Carlton,. and Kristin
C.\ Gunsalus at New York University: the Genomics Sequencing Core used
to enable entry, storage and analysis of genomic sequencing data,
implemented in OCaml.

Since the sequencing of the human genome in 2000, modern sequencing
equipment has been getting faster at high rates: Whereas storage costs
per amount of data are halving every 14 months, the data that
accumulates in genome sequencing doubles in volume every five months.
This places high demands on the computational infrastructure used to
process sequencing data.  In particular, where the sequencing
itself~-- performed by custom machines~-- used to dominate the time
spent in a typical genome-related project, the emphasis is migrating
to experimental design and datastream analysis.

The project described by Agarwal provides the computational
infrastructure for acceping and storing sequencing data and
sequencing, and making it accessible to distributed computations on a
compute cluster.  In particular, the Genomics Sequencing Core provides
an application server for managing the overall functioning of the
system, a job queue for computations that interfaces to the compute
cluster, and a web front end for the entire systems.  The system
maintains metadata racking samples, libraries and protocols.

The data that accumulates in the system is characterized by high
volume, but also by a high variety of different formats.  (The
velocity at which it arrives is not yet a problem, but may become a
challenge in the future.)  Agarwal and his colleages have developed a
DSL embedded into OCaml that is used to generate multiple
functionalities, among them serialization, SQL schemas, query scripts,
OCaml code for performing reads in inserts, web widgets, and diagrams.
This enables rapid development of functionality, and easy migration
between formats.

The Core also maintains a virtual filesystem that distinguishes
between ``original'' data and recomputes ``derived'' data on the fly.

The entire system is implemented in OCaml, using a wide variety of
libraries~-- in particular the
Ocsigen~\cite{Balat:2009:ERO:1596550.1596595} web framework, the Core
and Batteries libraries, and the Biocaml, PG'OCaml, Xmlm and Ocamlnet
libraries.  ``1.3 developers'' built the system and delivered the
first version in production within two months.  The experience with
OCaml has been mostly positive: Agarwal cited OCaml's
industrial-strength implementation, the availability of needed
libraries, and the excellent performance.  Agarwal mentioned the
complications maintaining a build system and the lack of ``blessed
libraries'' as factors that could still be improved.

Agarwal closed by summarizing the standing of functional programming
in biology: Functional programming is becoming a recognized term, and
thus the field is developing demand for software engineers who can
acquire domain knowledge and build software fast.  He noted that this
profile is different from that of data analysts, who need more
in-depth knowledge of the field and statistics.

\section{Using F\# to Prove Stabilization of Biological Networks}

Samin Ishtiaq reported on the \textit{Bio Model
  Analyzer}~\cite{Benque:2012:BVT:2362216.2362280} (or \textit{BMA},
online at \url{http://biomodelanalyzer.research.microsoft.com/})
developed by an interdisciplinary team at Microsoft Research: The Bio
Model Analyzer analyzes models from System Biology, which are
program-like descriptions of networks that describe systems like skin
or blood.

Particularly important to BMA are \textit{stability} properties: For
example, healthy skin should grow as many cells as it sheds.  If it
sheds more than it grows, sores develop and wounds do not heal well.
Growing more than it sheds is the definition of cancer.

The programs describing such biological networks are asynchronous
dataflow diagrams with typically tens of thousands of variables.  Each
variable has an associated update function that computes a new value
from the the values of other variables connected with it in the
network.  The objective of BMA is not just to simlulate a network, but
to prove general stability-related properties independent of a
particular starting state, such as the existence of a unique fixpoint,
several fixpoints or cycles.  This may be useful for developing new
drugs.

Traditional program-analysis tools do not scale or do not work for
programs with this many variables.  Consequently, BMA uses newly
developed techniques: It attempts to prove lemmas for small
subnetworks and propagate them through the entire networks in the hope
that enough lemmas propagate to prove stability.  The prover core does
not just report on the success of the proof search, but also allows
interactively stepping through the lemma-propagation process.  The
designers in the BMA team have targeted the UI at system biologists,
which use visualizations different from those computer scientist

BMA consists of three parts: The prover core written in F\#,
Microsoft's Z3 SMT solver written in C++ (but only linked into the
project), and the UI written in C\#.  The ``debugging functionality''
in the prover provides the propagation steps in a lazy sequence.  This
is very natural in F\#.  The UI is not written in F\#: in particular,
it benefits from the better tool support for C\#.

Ishtiaq had previously worked in OCaml using Emacs, and offered some
thoughts on the transition to F\# in Visual Studio: In particular,
Visual Studio offers interactive type checking and thus supports
exploratory programming in ways Emacs does not.  On the other hand,
OCaml still offers some higher-level abstraction mechanisms such as
GADTs, modules, and functors, which F\# still lacks.

\section{Developing an F\# Bioinformatics Application with HTML5
  Visualization}

Adam Granicz reported on joint work between University of Nebraska
Medical University and IntelliFactory on the \textit{functional
  genomics explorer} (or \textit{fgx}), which visualizes the genetic structure of a
Methicillin-Resistant Staphylococcus Aureus.  Methicillin-Resistant
Staphylococcus Aureus is more commonly known as \textit{MRSA}: MRSA
cause more deaths in the USA annually than HIV/AIDS.

The fgx software is a web application written in F\# using
IntelliFactory's \textit{WebSharper} web framework.  WebSharper
contains a transpiler from F\# to JavaScript, which allows the
complete application~--- with all client and server code~--- to be
written in F\#.  The fgx software uses a HTML5 canvas for drawing, and
interfaces to the the Krona JavaScript visualization
library~\cite{OndovBergmanPhilippy2011} through an F\# wrapper.  The
data that fgx visualizes is too large to transport to the web browser
in its entirety: fgx uses a combination of RPC calls and streaming via
WebSocket to transfer it incrementally.

Plans are underway to extend the software to a full Laboratory
Information Mangagement System, which would track and manage
sequencing experiments, samples, tools, and resources used.

\section{Developing Medical Software in Scala and Haskell}

Stefan Wehr reported on software developed by factis Research in
Freiburg for managing electronic patient records: Doctors can access
and enter patient information on tablets, which synchronize with a central
server.

The software needs to deal with significant amounts of data: Per day,
a hospital department with 170 patients generates about five lab
reports per patient per day. (20 in intensive care.)  Also, the
department produces about 150 images. (More than 1000 if CTs or MRI
scans are made.)  Also, the hospital's IT systems produce 34,000 HL7
messages.  (HL7 is a set of interoperability standards for healthcare
IT systems.)  All in all, the software needs to synchronize about
13~Megabytes per patient per day.

The tables serve as simple data viewers and entry points, with little
domain-specific knowledge of the healthcare application domain itself.
They are able to function offline, with periodic synchronization with
a \textit{synchronization server}, written in Haskell.  The
synchronization server generates documents from standardized import
data, and regenerates them if the input data changes.  It then
transfers the documents and images to the tablets, and receives and
processes data from them, keeping track of the synchronization state.
These server components also contain only little domain-specific code.

The domain-specific code resides in a separate \textit{data server},
written in Scala, which connects to the hospital's IT systems using
various protocols and APIs such as HL7, SQL, DICOM SAP.  It
communicates with the synchronization server via the
\textit{roundtrip} library, based on invertible syntax
descriptions~\cite{Rendel:2010:ISD:1863523.1863525}.  factis developed
roundtrip for Haskell and then ported it to Scala.

Generally, the experience with using functional programming for this
project has been expectedly positive.  factis (with four employees and
six freelancers) has been working on the application since 2010.  By
now, the synchronization server has about 69,000loc, and the data
server about 37,0000loc.

Wehr cited as Haskell's advantages the very expressive, rich static
type system, fine-grained control over side effects, ``immutability by
default'', very good support for testing, the excellent support for
concurrency, as well as an active and helpful community.  Haskell's
laziness occasionally proved problematic, with space profiling
providing only partial solutions.  Also, cabal proved problematic.
Wehr also reported that Haskell had a steep learning curve for
developers new to Haskell.

factis chose Scala for the data server because many Java APIs were
available that were helpful in integrating existing hospital IT
infrastructure.  Scala also has a very expressive, rich static type
system, favors immutability, good support for testing, a powerful yet
simple build system (sbt), direct access to the Java API, and powerful
monitoring facilities through Java ecossystem.  Java programmers find
it quite easy to adopt Scala.  On the other hand, Scala provides no
static control over side effects.  Subtyping makes Scala's type system
quite complex, which occasionally makes the type system difficult to
control.  Also, the easy migration path for Java programmers is a
double-edged sword, as Scala programmers coming from Java do not
always adopt functional programming as completely as they should.

All in all, Wehr concluded that both Haskell and Scala are excellent
languages for commercial software development.

\section{Functional programs connected to the power grid}

Sebastian Egner of Entelios AG, Berlin, reported on Entelios's project
on coordinated reduction of electrical loads at industrial production
facilities.  Entelios is a VC-funded startup founded in 2010 with
currently 20 employees.

In order to maintain stability in the German electrical system, the
\textit{Transmission System Operators} (TSOs---the companies that
maintain the network itself) balance supply and demand of electricity.
This is increasingly becoming a challenge with the advent of
electricity from renewable sources, particularly solar and wind
energy.  While these sources were able to provide 20\% of Germany's
elecricity demands on a single day in 2012, their supply is subject to
fluctuations and complex regulation.

Entelios provides \textit{operating reserve power}---the electricity
reserve needed to maintain stability---to the network by
\textit{demand-response management}, particularly by cooperating with
big consumers of electricity in the 100kW--100MW range such as arc
furnaces or paper mills.  These consumers can be switched off for
limited periods of time, thus providing ``negative consumption''
equivalent to positive production of electricity.  Entelios is
currently prequalified to provide operating-reserve power to all four
TSOs.

At consumers, Entelios installes ``EBoxes'', small embedded systems
connected to the Internet that interface with the control systems of
the consumers.  The software at Entelios's office controls the EBoxes.
In particular, Entelios maintains two redundant \textit{Network
  Operation Centers} at its two offices, with back-office software for
maintaining communication with the EBoxes and front-office software
for user interaction.  Both components are subject to frequently
changing requirements as the regulatory framework and market
environment changes.

The back-office software was originally written in Python, which was
available on the embedded systems at the heart of the EBoxes.  While
Python worked well enough at first, software had difficulty keeping up
with changing requirements, however, especially with unanticipated
changes in the sampling rate.  Python's threading facilities had
trouble keeping up with the soft real-time requirements.  As a result,
the back-end software has been rewritten in Erlang.

The front-office software is a rich client running on Windows, written
in F\# using the WPF framework.  Development of the front office was
outsourced to a company who are experts in \textit{functional-reactive
  programming}~\cite{Elliott:2009:PFR:1596638.1596643}, which became
the paradigm of the software for managing time-series data.

Experience with using functional programming has generally been
positive and enabled Entelios to roll out new features and react to
changing requirements quickly.  In particular, moving from Python to
Erlang showed a striking contrast: Whereas Python required substantial
work going from prototype to production-ready software, prototypes in
Erlang were usually quite close to production already.  Moving from
Python to Erlang required moderate deployment effort: Entelios found
two bugs in the Erlang system that have since been pushed upstream.
The experience of F\# was more mixed: While F\# is an effective
language for developing WPF applications, new developers found it
difficult to start working on the code base---F\#'s notational density
and rich interface to .NET often make it difficult to see which
language feature a particular piece of code exercises.  Also,
differences between the production and debugging environments made it
difficult to weed out some time and space leaks, which required
withdrawing and redoing several releases.

\section{Clojure iPad analytics dashboard in the energy sector}

Kevin Lynagh of Keming Labs described the approach his consultancy has
been taking towards visualizing data.  Keming Lab's mission is to make
data that such as log files that are not immediately fit for human
understanding accessible for non-technical people.  

Lynagh demonstrated two visualization applications he had implemented:
The \textit{o8}
framework\footnote{\url{https://github.com/chapmanb/o8}}, done for the
Harvard School for Public Health, provides a web-based platform to
interactively explore and analyze human variation data.  The other
application was an analytics dashboard for a farm of wind turbines,
which visualizes the status of the wind turbines in a form easily
accesible for maintenance personnel.

Both of these applications werde implemented using Lynagh's
\textit{C2} data visualization
library\footnote{\url{http://keminglabs.com/c2/}}, which is written in
Clojure and ClojureScript.  C2 enables developers to deploy their
visualization applications either on the web server or on the client.
For the client, ClojureScript compiles the code to JavaScript that
runs in the browser.

Using a browser for visualization has a number of advantages: it
provides scalable vector graphics, CSS, a scenegraph encoded in the
DOM, and many tools, all platform-independent.  ClojureScript allows
working around JacaScript's quirks, and provides rich data structures,
namespaces, and consistency.  Clojure's design was an important factor
in the design of C2 itself, as all data structures immutable by
default, and thus encourage the developer to think explicitly about
state.

In the case of the wind-turbine farm, the essential state is just
state of the wind turbines.  C2 allows transforming this state to a
visualization by a pure function generating DOM fragments.  The C2
framework then synchronizes the generated HTML code with the actual
DOM in the browser, keeping the visualization consistent with the
state.  This decouples the specification of the underlying application
from the visualization.  (It also couples markup and software
development, which may not be compatible with all design/development
workflows.)  C2 thus provides a clear dataflow in one direction, which
makes development easier than with direct manipulation of the DOM.
C2's method is inherently slower than direct DOM manipulation, but the
difference is irrelevant for many applications.

\section{The Awesome Haskell FPGA Compiler}

Peter Braams of Parallel Scientific described the benefits of using
the \textit{Haskell Hardware Description Language (HHDL)} for
programming FPGAs.  HHDL (designed by Serguey Zefirov) is a DSL
embedded in Haskell.  HHDL programs can either directly in the Haskell
run-time environment or be compiled to Verilog code and deployed on
the FPGA.  Using HHDL instead of Verilog directly allows FPGA
programmers to design at the conceptual level, and shorten the
test/design cycle significantly.  HHDL provides a rich set of types,
and the ensuing type safe helps assure the correctness of HHDL
programs.  The initial version of HHDL took merely five months to
develop.

Parallel Scientific uses HHDL to program an Arista Ethernet switch,
which contains an FPGA directly connected to eight 10~GB Ethernet
ports.  As the FPGA sits directly in the data path, it can receive and
thus process the Ethernet traffic faster than a separate CPU over a
traditional bus.  (The data rate on the ports exceeds the bandwith on
a PCIe, for example.)  HHDL allows building combinators to parse and
build packets, which can be alyered to build complex algorithms.  In
particular, Parallel Scientific is using HHDL on the switch to build a
ticker plant for financial exchanges, which aggregates and then
normalizes and maintains market data from multiple sources.  For
ticker plants, performance is crucial.  Parallel Scientific's solution
uses both the regular CPU in the switch for command and control as
well as the FPGA to receive and normalize the financial data.  The
resulting application is able to keep up with the port traffic in real
time.

\section{Conclusion}

We would like to thank Simon Thompson and Duncan Coutts for helping to
organize the CUFP tutorials, and Ashish Agarwal for organizing the BoF
sessions.  We thank Kresten Krab Thorup and Yaron Minsky for providing
notes on their talks.  Also, we thank Peter Thiemann and Fritz Henglein and the
whole ICFP/CUFP team for their assistance in Copenhagen.  We look
forward to CUFP 2013 in Boston!

\bibliography{biblio}
\end{document}

% end of JFP2egui.tex
